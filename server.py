import os
import io
import cv2
import torch
import numpy as np
import gc
import logging
from PIL import Image
from ultralytics import YOLO, SAM
from fastapi import FastAPI, File, UploadFile, Response, Form
from fastapi.middleware.cors import CORSMiddleware
import torch.hub

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"]
)

# ==============================================================================
# ğŸ’¡ [ì¡°ì • ê°€ëŠ¥í•œ ì„¤ì •] - Wall/Object Estimation Parameters
# ==============================================================================
# 1. YOLOv8 ê°ì²´ ê°ì§€ ë¯¼ê°ë„: ë‚®ì¶œìˆ˜ë¡ ë” ë§ì€ ê°ì²´ë¥¼ ê°ì§€í•˜ì—¬ ë²½ ì˜ì—­ì—ì„œ ì œì™¸ 
YOLO_CONF_THRESHOLD = 0.05 
# 2. ë„ˆë¬´ ì‘ì€ ê°ì²´ ë°•ìŠ¤ í•„í„°ë§ ê¸°ì¤€: ë‚®ì¶œìˆ˜ë¡ ì‘ì€ ê°ì²´ê¹Œì§€ í¬í•¨í•˜ì—¬ ì œì™¸
MIN_BOX_RATIO = 0.01
# 3. ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬ ì‹œ ì‚¬ìš©í•  ëª¨í´ë¡œì§€ ì»¤ë„ í¬ê¸°: í´ìˆ˜ë¡ ì •ì œ íš¨ê³¼ê°€ ê°•í•¨
MORPHOLOGY_KERNEL_SIZE = 9
# 4. ìµœì¢… ë§ˆìŠ¤í¬ ê²½ê³„ì˜ Gaussian Blur í¬ê¸°: í´ìˆ˜ë¡ ê²½ê³„ê°€ ë” ë¶€ë“œëŸ¬ì›€ 
GAUSSIAN_BLUR_SIZE = 13
# 5. ê¹Šì´ ë§µ ê¸°ë°˜ ê°ì²´ ì œê±° ë¯¼ê°ë„: ì´ ê°’ë³´ë‹¤ ê¹Šì´ ì°¨ì´ê°€ í¬ë©´ ê°ì²´ë¡œ ê°„ì£¼ (ë‚®ì¶œìˆ˜ë¡ ë¯¼ê°)
DEPTH_DIFF_THRESHOLD = 15 # 0-255 ìŠ¤ì¼€ì¼ì˜ ê¹Šì´ ë§µì—ì„œ ê²½ê³„ ì°¨ì´ ê¸°ì¤€

# ì „ì—­ ë³€ìˆ˜
det_model = None  # YOLOv8n
sam_model = None  # MobileSAM
midas_model = None # MiDaS for Monocular Depth Estimation
midas_transform = None # MiDaS input transformation
device = "cpu"


@app.on_event("startup")
def load_models_on_startup():
    """ì„œë²„ ì‹œì‘ ì‹œ YOLOv8n + MobileSAM + MiDaS ë¡œë“œ"""
    global det_model, sam_model, midas_model, midas_transform, device
    
    logger.info("[ğŸ”¥] Starting model loading for YOLOv8n + MobileSAM + MiDaS...")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"[âš™ï¸] Device: {device}")
    
    yolo_checkpoint_path = "yolov8n.pt"  
    sam_checkpoint_path = "mobile_sam.pt"

    try:
        # 1. YOLOv8n ëª¨ë¸ ë¡œë“œ
        if not os.path.exists(yolo_checkpoint_path):
             logger.error(f"[âŒ] YOLOv8n checkpoint not found at: {yolo_checkpoint_path}")
        else:
            det_model = YOLO(yolo_checkpoint_path)
            det_model.to(device)
            logger.info("[âœ…] YOLOv8n loaded.")
        
        # 2. MobileSAM ë¡œë“œ
        if not os.path.exists(sam_checkpoint_path):
             logger.error(f"[âŒ] MobileSAM checkpoint not found at: {sam_checkpoint_path}")
        else:
            sam_model = SAM(sam_checkpoint_path)
            sam_model.to(device)
            logger.info("[âœ…] MobileSAM loaded.")
            
        # 3. MiDaS ëª¨ë¸ ë¡œë“œ (MiDaS_small ì‚¬ìš©)
        midas_type = "MiDaS_small"
        midas_model = torch.hub.load("intel-isl/MiDaS", midas_type, trust_repo=True)
        midas_model.to(device)
        midas_model.eval()
        
        # MiDaS ëª¨ë¸ì— ë§ëŠ” ì…ë ¥ ë³€í™˜(Transform) í•¨ìˆ˜ ë¡œë“œ
        midas_transforms_module = torch.hub.load("intel-isl/MiDaS", "transforms", trust_repo=True)
        if midas_type == "MiDaS_small":
            midas_transform = midas_transforms_module.small_transform
        else:
            # DPT-Hybrid ë“± ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•  ê²½ìš°:
            midas_transform = midas_transforms_module.dpt_transform
            
        logger.info(f"[âœ…] MiDaS ({midas_type}) loaded.")

    except Exception as e:
        logger.error(f"[âŒ] FATAL Model loading failed: {e}", exc_info=True)


def np_from_upload(file_bytes: bytes, mode="RGB") -> Image.Image:
    """ë°”ì´íŠ¸ë¥¼ PIL Imageë¡œ ë³€í™˜"""
    try:
        return Image.open(io.BytesIO(file_bytes)).convert(mode)
    except Exception as e:
        logger.error(f"Failed to open image from bytes: {e}")
        return None

# ==============================================================================
# --- MiDaS ê¹Šì´ ë§µ ìƒì„± í•¨ìˆ˜ ---
# ==============================================================================
def generate_depth_map_midas(pil_img: Image.Image, output_size: tuple) -> np.ndarray:
    """
    MiDaS ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ RGB ì´ë¯¸ì§€ë¡œë¶€í„° ê¹Šì´ ë§µì„ ì¶”ì •í•©ë‹ˆë‹¤.
    """
    if midas_model is None or midas_transform is None:
        logger.error("MiDaS model or transform not initialized.")
        return None

    try:
        # 1. MiDaS ì…ë ¥ ë³€í™˜ ì ìš©
        input_batch = midas_transform(pil_img).to(device)
        
        with torch.no_grad():
            # 2. MiDaS ëª¨ë¸ ì‹¤í–‰
            prediction = midas_model(input_batch)
            
            # 3. ì¶œë ¥ í¬ê¸°ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ì— ë§ê²Œ ì¡°ì •
            prediction = torch.nn.functional.interpolate(
                prediction.unsqueeze(1),
                size=pil_img.size[::-1], # (H, W)
                mode="bicubic",
                align_corners=False,
            ).squeeze()
        
        # 4. NumPyë¡œ ë³€í™˜ ë° ì •ê·œí™”
        depth_map = prediction.cpu().numpy()
        
        # 5. ê¹Šì´ ë§µì„ 0-255 ìŠ¤ì¼€ì¼ë¡œ ì •ê·œí™” (Occlusion Mask ìƒì„±ì— í™œìš©í•˜ê¸° ìœ„í•¨)
        depth_min = depth_map.min()
        depth_max = depth_map.max()
        
        if depth_max - depth_min > 0:
            normalized_depth = (depth_map - depth_min) / (depth_max - depth_min)
        else:
            normalized_depth = np.zeros_like(depth_map)

        # 0-255 ë²”ìœ„ì˜ 8ë¹„íŠ¸ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜
        normalized_depth_uint8 = (normalized_depth * 255).astype(np.uint8)
        
        logger.info("[âœ…] MiDaS ê¹Šì´ ë§µ ìƒì„± ì™„ë£Œ.")
        return normalized_depth_uint8

    except Exception as e:
        logger.error(f"MiDaS depth generation failed: {e}", exc_info=True)
        return None


def filter_small_boxes(boxes, img_shape, min_ratio=MIN_BOX_RATIO):
    """ë„ˆë¬´ ì‘ì€ ë°•ìŠ¤ í•„í„°ë§ (ë…¸ì´ì¦ˆ ì œê±°)."""
    H, W = img_shape
    area_img = H * W
    filtered = []
    for x1, y1, x2, y2 in boxes:
        area = (x2 - x1) * (y2 - y1)
        # ë©´ì ì´ ì „ì²´ ì´ë¯¸ì§€ì˜ min_ratio ë¯¸ë§Œì´ë©´ í•„í„°ë§
        if area / area_img > min_ratio:
            filtered.append([float(x1), float(y1), float(x2), float(y2)])
    return filtered


def post_refine(mask: np.ndarray):
    """ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬: ë…¸ì´ì¦ˆ ì œê±°, í™•ëŒ€, ê°€ì¥ í° ì—°ê²° ì˜ì—­ë§Œ ë‚¨ê¸°ê¸° (ë²½ ì˜ì—­ ì¶”ì •)."""
    mask = mask.astype(np.uint8)
    kernel = np.ones((MORPHOLOGY_KERNEL_SIZE, MORPHOLOGY_KERNEL_SIZE), np.uint8)

    # ë…¸ì´ì¦ˆ ì œê±° (Opening) + ê²½ê³„ ì±„ìš°ê¸° (Dilate)
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=1)
    mask = cv2.dilate(mask, kernel, iterations=1)

    # ê°€ì¥ í° ì—°ê²° ì˜ì—­ë§Œ ë‚¨ê¸°ê¸°
    cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not cnts:
        return mask

    largest = max(cnts, key=cv2.contourArea)
    clean = np.zeros_like(mask)
    cv2.drawContours(clean, [largest], -1, 1, thickness=cv2.FILLED)
    
    # ì˜ì—­ì„ ë¶€ë“œëŸ½ê²Œ ë‹«ê¸° (Closing)
    clean = cv2.morphologyEx(clean, cv2.MORPH_CLOSE, kernel, iterations=2)
    return clean


def create_depth_occlusion_mask(depth_map: np.ndarray, threshold=DEPTH_DIFF_THRESHOLD) -> np.ndarray:
    """
    ê¹Šì´ ì§€ë„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ê²½ ê°ì²´(Occlusion) ë§ˆìŠ¤í¬ ìƒì„±.
    ì¸ì ‘ í”½ì…€ ê°„ì˜ ê¸‰ê²©í•œ ê¹Šì´ ë³€í™”(ê²½ê³„)ë¥¼ ì°¾ì•„ ê°ì²´ë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    """
    if depth_map is None:
        return None
        
    depth_map = depth_map.astype(np.float32)
    
    # Sobel í•„í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¹Šì´ ë§µì˜ ê²½ê³„(ê¹Šì´ ë³€í™”ê°€ í° ë¶€ë¶„)ë¥¼ ê²€ì¶œ
    grad_x = cv2.Sobel(depth_map, cv2.CV_32F, 1, 0, ksize=3)
    grad_y = cv2.Sobel(depth_map, cv2.CV_32F, 0, 1, ksize=3)
    
    # ê²½ê³„ ê°•ë„ ê³„ì‚° (Magnitude)
    magnitude = cv2.magnitude(grad_x, grad_y)
    
    # ì„ê³„ê°’ ì´ìƒì˜ ê²½ê³„ë§Œ ë§ˆìŠ¤í‚¹ (ê°ì²´ = 1, ë°°ê²½ = 0)
    occlusion_mask = (magnitude > threshold).astype(np.uint8)
    
    # ë§ˆìŠ¤í¬ í™•ì¥ (dilate)í•˜ì—¬ ê°ì²´ ì˜ì—­ì„ í™•ì‹¤í•˜ê²Œ ë®ìŠµë‹ˆë‹¤.
    kernel = np.ones((5, 5), np.uint8)
    occlusion_mask = cv2.dilate(occlusion_mask, kernel, iterations=2)
    
    return occlusion_mask


@app.get("/")
async def root():
    return {"status": "ok", "message": "YOLOv8n + MobileSAM + MiDaS Integrated Server"}


@app.get("/health")
async def health():
    import psutil
    process = psutil.Process()
    memory_mb = process.memory_info().rss / 1024 / 1024
    
    gc.collect()
    
    return {
        "status": "healthy",
        "models_loaded": det_model is not None and sam_model is not None and midas_model is not None,
        "device": device,
        "memory_mb": round(memory_mb, 2)
    }


@app.post("/segment_wall_mask")
async def segment_wall_mask(
    rgb_file: UploadFile = File(..., alias="rgb_file"), # ìœ ë‹ˆí‹° ì¹´ë©”ë¼ ì´ë¯¸ì§€
    depth_file: UploadFile = File(..., alias="depth_file") # ìœ ë‹ˆí‹° ê¹Šì´ ì§€ë„ (í‘ë°± PNG ê°€ì •)
):
    """YOLOv8n+SAMìœ¼ë¡œ ê°ì²´ ê°ì§€/ë¶„í•  í›„, MiDaS ë˜ëŠ” ì‹¤ì œ ê¹Šì´ ì§€ë„ë¡œ ìµœì¢… ê°€ë ¤ì§ ë§ˆìŠ¤í¬ë¥¼ ì ìš©í•˜ì—¬ ë²½ ì˜ì—­ ì¶”ì¶œ"""
    
    # ëª¨ë¸ ë¡œë”© ì—¬ë¶€ í™•ì¸
    if det_model is None or sam_model is None or midas_model is None:
        logger.error("Segmentation services are unavailable due to model loading failure.")
        return Response(content="Model load failed. Check server startup logs.", status_code=503)

    img = pil_img = results = boxes = sam_boxes = depth_img_np = depth_occlusion_mask = None 

    try:
        # 1. RGB ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
        rgb_bytes = await rgb_file.read()
        pil_img = np_from_upload(rgb_bytes, mode="RGB")
        if pil_img is None:
            logger.error("RGB file could not be loaded.")
            return Response(content="Invalid RGB image file.", status_code=400)
            
        original_size = pil_img.size
        
        max_size = 640
        if max(pil_img.size) > max_size:
            ratio = max_size / max(pil_img.size)
            new_size = tuple(int(dim * ratio) for dim in pil_img.size)
            pil_img = pil_img.resize(new_size, Image.LANCZOS)

        w, h = pil_img.size
        logger.info(f"[ğŸ“¸] RGB ì´ë¯¸ì§€: {w}x{h}")
        
        # 2. ê¹Šì´ ì§€ë„ ë¡œë“œ ë° MiDaS í´ë°± ì ìš©
        depth_bytes = await depth_file.read()
        
        # í´ë¼ì´ì–¸íŠ¸ì—ì„œ ë³´ë‚¸ ê¹Šì´ ë°ì´í„°ê°€ ìœ íš¨í•œì§€ í™•ì¸ (ë¹ˆ PNGëŠ” 100ë°”ì´íŠ¸ ë¯¸ë§Œì¼ ìˆ˜ ìˆìŒ)
        if depth_bytes and len(depth_bytes) > 100: 
            # 2-1. í´ë¼ì´ì–¸íŠ¸ì˜ ì‹¤ì œ ê¹Šì´ ë°ì´í„° ì‚¬ìš©
            depth_img = np_from_upload(depth_bytes, mode="L")
            if depth_img is not None:
                depth_img = depth_img.resize((w, h), Image.NEAREST) 
                depth_img_np = np.array(depth_img)
                logger.info("[âœ…] í´ë¼ì´ì–¸íŠ¸ ê¹Šì´ ì§€ë„ ë¡œë“œ ì™„ë£Œ.")
            else:
                 # ê¹Šì´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ì‹œ MiDaS í´ë°±
                logger.warning("[âš ï¸] í´ë¼ì´ì–¸íŠ¸ ê¹Šì´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. MiDaSë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                depth_img_np = generate_depth_map_midas(pil_img, (w, h))
        else:
            # 2-2. í´ë¼ì´ì–¸íŠ¸ ê¹Šì´ ë°ì´í„°ê°€ ì—†ì„ ê²½ìš° MiDaS ì‚¬ìš© (í´ë°±)
            logger.warning("[âš ï¸] í´ë¼ì´ì–¸íŠ¸ ê¹Šì´ íŒŒì¼ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. MiDaSë¡œ ê¹Šì´ ë§µì„ ìƒì„±í•©ë‹ˆë‹¤.")
            depth_img_np = generate_depth_map_midas(pil_img, (w, h))


        # 3. YOLOv8n + MobileSAMìœ¼ë¡œ ì´ˆê¸° ë²½ ë§ˆìŠ¤í¬ ìƒì„±
        logger.info("[ğŸ”] YOLOv8n: ê°ì²´ ê°ì§€ ì¤‘...")
        results = det_model.predict(
            pil_img, conf=YOLO_CONF_THRESHOLD, imgsz=640, device=device, verbose=False,
        )[0]
        xyxy = results.boxes.xyxy.cpu().numpy() if results.boxes is not None else []
        boxes = filter_small_boxes(xyxy, pil_img.size[::-1])
        logger.info(f"[âœ…] {len(boxes)}ê°œì˜ ìœ íš¨ ê°ì²´ ë°•ìŠ¤ ë°œê²¬ (Threshold: {YOLO_CONF_THRESHOLD})")

        if not boxes:
            logger.warning("[âš ï¸] ê°ì²´ ë°•ìŠ¤ê°€ ì—†ì–´ ì „ì²´ ì´ë¯¸ì§€(ë²½) ë°•ìŠ¤ ì‚¬ìš©.")
            initial_wall_mask = np.ones((h, w), dtype=np.uint8) * 255
        else:
            logger.info("[ğŸ¨] MobileSAM: ê°ì²´ ë¶„í•  ì¤‘...")
            sam_boxes = boxes
            res = sam_model.predict(
                pil_img, bboxes=sam_boxes, device=device, retina_masks=False, verbose=False
            )[0]

            if res.masks is None:
                logger.warning("[âš ï¸] MobileSAM ë¶„í•  ì‹¤íŒ¨. ì „ì²´ í™”ë©´ ë°˜í™˜.")
                initial_wall_mask = np.ones((h, w), dtype=np.uint8) * 255
            else:
                # ë§ˆìŠ¤í¬ í†µí•© ë° ë°˜ì „ (ë²½ ì˜ì—­ ì¶”ì¶œ)
                mask_data = res.masks.data.cpu().numpy()
                union_objects = (mask_data.sum(axis=0) > 0).astype(np.uint8)
                background_mask = 1 - union_objects # ê°ì²´ ë§ˆìŠ¤í¬ ë°˜ì „
                
                # í›„ì²˜ë¦¬ (ê°€ì¥ í° ë°°ê²½ ì˜ì—­ë§Œ ë‚¨ê¹€)
                refined_background = post_refine(background_mask) 
                initial_wall_mask = (refined_background * 255).astype(np.uint8)
                
                del mask_data, union_objects, background_mask, refined_background


        # 4. ê¹Šì´ ì§€ë„ë¥¼ ì´ìš©í•œ ìµœì¢… ê°ì²´ ì œì™¸ ë§ˆìŠ¤í‚¹ (Depth Occlusion)
        final_mask_img = initial_wall_mask.copy()
        
        if depth_img_np is not None:
            depth_occlusion_mask = create_depth_occlusion_mask(depth_img_np)
            
            # ê¹Šì´ ë§ˆìŠ¤í¬ë¥¼ ë°˜ì „í•˜ì—¬ ë²½ ë§ˆìŠ¤í¬(ë²½=1, ê°ì²´=0)ë¥¼ ì–»ê³  ê¸°ì¡´ ë§ˆìŠ¤í¬ì™€ AND ì—°ì‚°
            wall_from_depth = 1 - depth_occlusion_mask 
            
            # 2D AI ë§ˆìŠ¤í¬ì™€ 3D ê¹Šì´ ë§ˆìŠ¤í¬ë¥¼ ê²°í•© (ë‘ ë§ˆìŠ¤í¬ ëª¨ë‘ 1ì¸ ì˜ì—­ë§Œ ë‚¨ê¹€)
            combined_mask = cv2.bitwise_and(final_mask_img, wall_from_depth * 255)
            final_mask_img = combined_mask
            logger.info("[âœ…] ê¹Šì´ ë°ì´í„°(í´ë¼ì´ì–¸íŠ¸ or MiDaS)ë¡œ ìµœì¢… ê°€ë ¤ì§ ë³´ì • ì™„ë£Œ.")
            
            del wall_from_depth, combined_mask
        else:
            logger.warning("[âš ï¸] ê¹Šì´ ë°ì´í„°ê°€ ì—†ì–´ 2D AI ë§ˆìŠ¤í¬ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")


        # 5. ìµœì¢… ë§ˆìŠ¤í¬ ì •ë¦¬ ë° ì¸ì½”ë”©
        
        # ê²½ê³„ë©´ ë¶€ë“œëŸ½ê²Œ ì²˜ë¦¬ (Smoothing)
        final_mask_img = cv2.GaussianBlur(final_mask_img, (GAUSSIAN_BLUR_SIZE, GAUSSIAN_BLUR_SIZE), 0)
        
        # ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
        if pil_img.size != original_size:
            final_mask_img = cv2.resize(
                final_mask_img, 
                original_size, 
                interpolation=cv2.INTER_LINEAR
            )
        
        # PNG ì¸ì½”ë”©
        _, png = cv2.imencode(".png", final_mask_img)

        # ğŸš¨ ë©”ëª¨ë¦¬ ì •ë¦¬ ê°•í™” 
        del pil_img, results, boxes, sam_boxes, depth_img_np, depth_occlusion_mask
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache() 
        
        gc.collect() 

        final_png_bytes = png.tobytes()
        del png, _
        gc.collect()

        return Response(
            content=final_png_bytes,
            media_type="image/png",
            headers={
                "Access-Control-Allow-Origin": "*",
                "Cache-Control": "no-cache"
            }
        )

    except Exception as e:
        logger.error(f"âŒ ERROR in segmentation processing: {e}", exc_info=True)
        gc.collect()
        return Response(
            content=f"Internal Server Error: {e}".encode(),
            status_code=500
        )


@app.options("/segment_wall_mask")
async def options_segment_wall_mask():
    return Response(
        content=b'',
        status_code=200,
        headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "POST, OPTIONS",
            "Access-Control-Allow-Headers": "*"
        }
    )